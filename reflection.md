#Reflection
The prompts that co pilot genereated for me was 'def_remove_invalid_rows(df)' and 'def_handle_missing_values'(df). I used co pilot to make these changes because two functions seemed like they would take the longest to write. I prompted Co-pilot by including the raw data frame in my prompt and not being too specific in it. I asked it add theses two functions to my lines of code based on the csv file.

In co-pilots code that was filling in the price for missing data it originally used the mean, but i realized when reading the raw data vs the clean that it was giving numbers that did not seem realstic. For example for the wireless mouse product when using mean the price is 37 dollars while using median it was 7 dollars. While it did what it was supposed to logically it was incorrect.

Cleaning this dataset helped me to understanf how important it is to make sure you have cleaned and prepared data is before using it. At first the file did not look like it had anything wrong with it but as i stated working with it I realized it had a lot of hidden issues. And these could produce inaccurate result if i do not handle using them correctly. Some of the problems where that the formatting was not consistent, there was unessary quotation marks, missing numbers, negative numbers, and names that were not standardized. Working through these problems showed me how crucial it is to approach data in a structured way. Using co pilot also helped me learn that even though it will generate a useable answer it is best to check wheat it produces because it is not allways logically correct
